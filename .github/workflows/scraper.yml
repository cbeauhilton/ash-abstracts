name: Scraper

on:
  workflow_call:
    inputs:
      scraper: 
        type: string
        required: true
      commit_message:
        required: false
        type: string
    secrets:
      push_token:
        required: true

# on:
#   push:
#     branches:
#     - main
#   schedule:
#     - cron: '59 */3 * * *'

jobs:
  build:
    runs-on: ubuntu-latest
    if: contains(github.event.head_commit.message, ${{inputs.commit_message}}) || (github.event_name == 'schedule')
    steps:
    - name: Checkout this repo
      uses: actions/checkout@v2
    - name: Checkout ash-files
      uses: actions/checkout@v2
      with:
        repository: cbeauhilton/ash-files
        path: scrape_ash/scrape_ash/ash-files
        token: ${{ secrets.push_token }}
    - name: Set up Python
      uses: actions/setup-python@v1
      with:
        python-version: 3.9
    - uses: actions/cache@v2
      name: Configure pip caching
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/scraping-requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r scraping-requirements.txt
    - name: Make sure data directories exist
      run: |-
        cd scrape_ash/scrape_ash/
        mkdir -p data
        mkdir -p data/doi_json
        mkdir -p ash-files/doi_json
    - name: Download url database
      run: |-
        cd scrape_ash/scrape_ash/data/
        curl --fail -OL https://github.com/cbeauhilton/ash-db/releases/download/database/urls.db
      continue-on-error: true
    - name: Download abstract database
      run: |-
        cd scrape_ash/scrape_ash/data/
        curl --fail -OL https://github.com/cbeauhilton/ash-db/releases/download/database/abstracts.db
      continue-on-error: true
    - name: Scrape
      id: scrape
      run: |-
        cd scrape_ash/scrape_ash/
        echo "CLOSESPIDER_PAGECOUNT = 200" >> settings.py
        echo "CLOSESPIDER_ITEMCOUNT = 300" >> settings.py
        scrapy crawl ${{inputs.scraper}}
    - name: Commit scraped files
      if: steps.scrape.outcome == 'success'
      run: |-
        echo " " > .gitignore
        cd scrape_ash/scrape_ash/
        cp data/doi_json/*.json ash-files/doi_json/
        cd ash-files
        git config --global user.name "${{inputs.scraper}} bot"       
        git config --global user.email "actions@users.noreply.github.com"
        git add -A
        timestamp=$(date -u)
        git commit --allow-empty  -m "Latest ${{inputs.scraper}} data: ${timestamp}"
        git push
    - name: Upload to releases
      if: steps.scrape.outcome == 'success'
      uses: softprops/action-gh-release@v1
      with:
        repository: cbeauhilton/ash-db
        token: ${{ secrets.push_token }}
        tag_name: "database"
        files: |
          scrape_ash/scrape_ash/data/urls.db
          scrape_ash/scrape_ash/data/abstracts.db
